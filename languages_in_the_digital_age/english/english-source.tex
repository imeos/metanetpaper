
  \section{English in the European Information Society} 
  
  \subsection{General Facts}

  Around the world, there are around 375 million native speakers of English. As such, it is estimated to be the third largest language, coming behind only Mandarin Chinese and Spanish. English is a (co)-official language in 53 countries worldwide.   
    
Within Europe, English is the most commonly used language in the United Kingdom. It is not an official language in the UK, since there is no formal constitution. However, it can be considered the \textit{de facto} language, given that it is the official language of the British government, and is spoken by around 94\% of the 62 million inhabitants of the UK\cite{Leg1} . It is also the most widely spoken language in the Republic of Ireland (population approx. 4.5 million), where English is the second official language, after Irish.  English is additionally the official language of Gibraltar (a British overseas territory) and a co-official language in Jersey, Guernsey and the Isle of Man (British Crown Dependencies), as well as in Malta. Outside of Europe, the countries with the greatest number of native English speakers are the United States of America (215 million speakers), Canada (17.5 million speakers) and Australia (15.5 million speakers).
    
In addition to English, the UK has further recognised regional languages, according to the European Charter for Regional or Minority Languages (ECRML), i.e., Welsh, Scottish Gaelic, Cornish, Irish, Scots, and its regional variant Ulster Scots. Since February 2011, the Welsh language (which is spoken by approximately 20\% of the population of Wales) has shared official status with English in Wales \cite{Leg2}.  The large number of British Asians (approx 2.3 million or 4\% of the population, according to the 2001 census) give rise to other languages being spoken in the UK, most notably Punjabi and Bengali. 
    
\boxtext{English is a (co)-official language in 53 countries worldwide.}    
    
Due to global spread of English, a large number of dialects have developed. Major dialects such as American English and Australian English can be split into a number of sub-dialects. In recent times, differences in grammar between the dialects have become relatively minor, with major variations being mainly limited to pronunciation and, to some extent, vocabulary, e.g., \textit{bairn} (child) in northern England and Scotland.         
    
In addition to dialects, there are also a number of English-based pidgins and creole languages. Pidgins are simplified languages that develop as a means of communication between two or more groups that do not have a language in common. An example is Nigerian pidgin, which is a used as a \textit{lingua franca} in Nigeria, where 521 languages have been identified. A creole language is a pidgin that has become nativised (i.e. learnt as a native language), such as Jamaican Patois.
    
As further general reading on the English language, the reader is referred to: \cite{crystal1}, \cite{crystal2},\cite{bragg}, \cite{bryson}.

  \subsection{Particularities of the English Language}

  \ParallelLText{
   Compared to most European languages, English has minimal inflection, with a lack of grammatical gender or adjectival agreement. Grammatical case marking has also largely been abandoned, with personal pronouns being a notable exception, where nominative case (I, we, etc.), accusative/dative case (me, us, etc.) and genitive case (my, our, etc.) are still distinguished. 
   A particular feature of the English language is its spelling system, which is notoriously difficult to master for non-native speakers. Whilst in many languages, there is a consistent set of rules that map spoken sounds to written forms, this is not the case in English.  Nearly every sound can be spelt in more than one way, and conversely, most letters can be pronounced in multiple ways. Consequently, English has been described as “the world's worst spelled language”\cite{Laubach}.  
   Consider the \textit{/u:/} sound, which in English can be spelt (among other ways) as “oo” as in \textit{boot}, “u” as in \textit{truth}, “ui” as in {\textit fruit}, “o” as in \textit{to}, “oe” as in \textit{shoe}, “ou” as in \textit{group}, “ough” as in \textit{through} and “ew” as in \textit{flew}. Having multiple written ways to represent a single sound is not in itself an unusual feature of written languages. For example, the same sound can be written in French as “ou”, “ous”, “out” or “oux”. However, what is more unusual about English is the fact that most of the written forms have alternative pronunciations as well, e.g.  r\textit{\textbf{u}}b, b\textit{\textbf ui}ld, g\textit{\textbf{o}}, t\textit{\textbf{oe}}, \textit{\textbf{ou}}t, r\textit{\textbf{ough}}, s\textit{\textbf{ew}}. One of the most notorious amongst the groups of letters listed is \textit{ough}, which can be pronounced in up to ten different ways. 
   
   \boxtext{English has a notoriously difficult spelling system.}
   
   These special features of English are the result of a number of factors, including the complex history of the UK, which has been heavily influenced by previous invasions and occupations by Scandinavians and Normans. Also, English spelling does not reflect the significant changes in the pronunciation of the language that have occurred since the late fifteenth century. In contrast to many other languages, and despite numerous efforts, most efforts to reform English spelling have met with little success. 
   A further defining feature of English is the large number of phrasal verbs, which are combinations of verb and preposition and/or adverb. The meaning of phrasal verbs is often not easily predictable from their constituent parts, which make then an obstacle for learners of English. By means of an example, the verb get can occur in a number of phrasal verb constructions, such as \textit{get by} (cope or survive), \textit{get over} (recover from) and \textit{get along} (be on good terms).
   
    \boxtext{The meaning of English phrasal verbs is not easily predictable from their constituent parts.}   
     }
  
  \ParallelRText{}  
  
    \ParallelPar


  \subsection{Recent Developments}

  \ParallelLText{
    Events in the more recent history of the UK have had a significant influence on the vocabulary of English. These events include the industrial revolution, which necessitated the coining of new words for things and ideas that had not previously existed, and the British Empire. At its height, the empire covered one quarter of the earth's surface, and a large number of foreign words from the different countries entered the language. The increased spread of public education increased literacy, and, combined with the spread of public libraries in the 19th century, books (and therefore a standard language) were exposed to a much greater number of people. The migration of large numbers of people from many different countries to the United States of America also affected the development of American English. 
    The two world wars of the 20th century caused people from different backgrounds to be thrown together, and the increased social mobility that followed contributed to many regional differences in the language being lost, at least in the UK. With introduction of radio broadcasting, and later of film and television, people were further exposed to unfamiliar accents and vocabulary, which also influenced the development of the language. Today, American English has a particularly strong influence on the development of British English, due to the USA's dominance of cinema, television, popular music, trade and technology (including the Internet).
    
     \boxtext{The 20th century has seen the disappearance of many regional language differences in the UK.}    
     
         The online edition of the Oxford English Dictionary is updated four times per year, with the March 2011 release including 175 new words, many of which indicate the rapidly changing nature of our society \cite{oed}. These words include initialisms such as \textit{OMG} (Oh my god) and \textit{LOL} (Laughing out loud), which reflect the increasing influence of electronic communications (e.g., email, text messaging, social networks, blogs, etc.) on everyday lives. An increasing thirst for travel and cuisines of the word has caused loan words such as \textit{banh mi} (Vietnamese sandwich) to be listed. 
    
      \boxtext{The online Oxford English Dictionary is updated 4 times per year to accommodate the rapidly changing nature of the language.}                
    Within Europe, English can today be considered the most commonly used language, with 51\% of EU citizens speaking it either as a mother tongue or a foreign language, according to EUROBAROMETER survey \cite{europa} .  Considering non-native speakers of English in the EU, 38\% state that they have sufficient English skills to hold a conversation. English is the most widely known language apart from the mother tongue in 19 of the 29 countries polled, with particularly high percentages of speakers in Sweden (89\%), Malta (88\%) and the Netherlands (87\%).
    
       \boxtext{51\% of EU citizens speak English as another tongue or foreign language.}    
  }
  
  \ParallelRText{
      }

  \ParallelPar


  \subsection{Language Cultivation in The UK}

  \ParallelLText{
    There are a number of associations, both nationally and internationally, which aim to promote the English language. These include the English Association \cite{EngAssoc}, which was founded in 1906, with the aim of furthering knowledge, understanding and enjoyment of the English language and its literature, and to foster good practice in its teaching and learning at all levels. The Council for College and University English \cite{ccue}  and the National Association for the Teaching of English \cite{nate}  promote standards of excellence in the teaching of English at different levels, from early years through to university studies. The European Society for the Study of English \cite{essen}  promotes the study and understanding of English languages, literature, and cultures of English-speaking people within Europe. 
    The Queen's English Society \cite{qes}  (QES) is a charity founded in 1972, which aims to protect the English language from perceived declining standards. Its objectives include the education of the public in the correct and elegant usage of English, whilst discouraging the intrusion of anything detrimental to clarity or euphony. Such intrusions include the introduction of “foreign” words and, in recent years, new technologies such as internet chat and text messaging. As such, the aims of the QES appear to be in conflict with those of the Oxford English Dictionary, which aims to describe recent changes in the language, rather than taking a prescriptive view of what is correct.  
    The aims of the QES are not so different from those of the language academies that exist in other European countries (e.g., L'Acad\'{e}mie Fran\c{c}aise in France, the Real Academia Espa\~{n}ola in Spain and the Accademia della Crusca in Italy). These academies determine standards of acceptable grammar and vocabulary, as well as adapting to linguistic change by adding new words and updating the meanings of existing ones. Indeed, in 2010, it was attempted to form an Academy of English using a similar model to the academies listed above.  However, such a prescriptive approach generated a large amount of bad press concerning objections to the suppression of linguistic diversity and evolution. Consequently, the project was abandoned after a few months.   
        
    
        }
    
  \ParallelRText{
     }

  \ParallelPar
  

  \subsection{Language in Education}

  \ParallelLText{
    From the early 1960s until 1988, there was little or no compulsory English grammar teaching in schools. The Education Reform act of 1988, and with it the introduction of the National Curriculum, has resulted in greater structure in the teaching of English in the UK, including the re-introduction of grammar as a required element. From ages 5-16, during which the study of English is a compulsory subject (except in Wales), the teaching requirements are divided into the key areas of listening, speaking, reading and writing \cite{curric}. The study of language structure, as well both standard English and variations (including dialects), and culture, are an integral part of each of the key areas, and are developed throughout the learning process. Between 2003 and 2010, the study of a foreign language was only compulsory between the ages of 11-14, causing a 30\% drop in the number of students opting to study a foreign language beyond 14. However, from 2010, foreign language learning was planned to begin at age 10.   
    From the age of 16, education in the UK is optional. A 2006 survey of subjects studied by 16-18 year olds in England found that English literature was the third most popular subject (after General Studies and Mathematics)\ cite{camb}, studied by approximately 19.5\% of students. In contrast, only 7\% per cent of students opt to study English language, making it the 14th most popular subject. This still puts it above the two most popular foreign languages, i.e., French at 22nd position (5\% of students) and German at 29th position (2\% of students).  At degree level in UK universities, English ranked as the 6th most popular subject in 2010, with a small increase in applications (8.6\%) compared to 2009.
     PISA studies \cite{pisa} measure reading literary skills amongst teenagers in different countries. According to the results, UK students are failing to improve at the same rate as students in some other countries. Although the overall scores of UK teenagers have not altered significantly between 2000 and 2009, their performance compared to other participating countries has dropped from 7th to 25th position. According to the amount spent per student on teaching, the UK ranks 8th among the 65 countries taking part. The overall literacy score for the UK  is not statistically significant from the average score of all participant countries, and as such has comparable rates of teenage literacy to countries such as France, Germany and Sweden and Poland. In the 2009 study, around 18\% of UK students did not achieve the basic reading level.    
     In the PISA studies, a major factor influencing reading performance variability between schools was found to be the socio-economic background of the students. The UK has quite a large percentage of immigrant students, with around 200 different native languages being represented at British schools \cite{Leg1} . However, there is generally a small gap between the performance of natives and immigrants. Although immigrants who do not speak English at home have considerably reduced skills, children whose native language is not English receive linguistic support to enable them to attain the minimum level of understanding and expression to follow their studies.
     Within Europe, English is the most studied foreign language within schools, with a study carried out by Eurydice \cite{ecea}  revealing that 90\% of all European pupils learn English at some stage of their education. It is the mandatory first foreign language in 13 countries of Europe. 
     
      \boxtext{90\% of all European pupils learn English at some stage of their education.}
           
    }
     
    

  \ParallelRText{
  }
  
  \ParallelPar


  \subsection{International Aspects}

  \ParallelLText{
    Driven by both British imperialism and the ascension of the USA as a global superpower since the Second World War, English has been increasingly developing as the \textit{lingua franca} of global communication. It is the dominant or even the required language of communications, science, information technology, business, aviation, entertainment, radio and diplomacy, and a working knowledge of English has become a requirement in a number of fields, occupations and professions such as medicine and computing. As a consequence of this, over a billion people now speak English, at least to a basic level. Within the European Union, English is one of the three working languages of the European Commission (together with French and German). It is also one of the six official languages of the United Nations.  
    
     \boxtext{English has been increasingly developing as the \textit{lingua franca} of global communication.}
     
         In science, the dominant nature of English can be viewed in two ways. On the one hand, its use as a common language in scientific publishing allows for ease of information storage and retrieval, and for knowledge advancement. On the other hand, English can be seen as something of a\textit{Tyrannosaurus rex }- “a powerful carnivore gobbling up the other denizens of the academic linguistic grazing grounds” \cite{swales} . Scientists face a great deal of pressure to publish in visible (usually international) journals, most of which are now in the English language, leading to a self-perpetuating cycle in which English is becoming increasingly important. 
    
The global spread of English is creating further negative impacts, e.g., the reduction of native linguistic diversity in many parts of the world. Its influence continues to play an important role in language attrition. 

  \boxtext{The global spread of English is reducing linguistic diversity in many parts of the world.}  }
  
  \ParallelRText{
      }
  
  \ParallelPar


  \subsection{English on the Internet}

  \ParallelLText{
  In 2010, 30.1 million adults in the UK (approximately 60\%) used the Internet almost daily, which is almost double the estimate of 2006 \cite{stats1}.  The same report found that 19.1 million UK households (73\%) had an Internet connection. It was found that Internet use is linked to various socio-economic and demographic indicators. For example, 60\% of users aged 65 or over had never accessed the Internet, compared to 1\% of those ages 16 to 24. Educational back-ground also has an impact on Internet use. Some 97\% of degree-educated adults had used the Internet, compared to 45\% of people without formal qualifications.  
  In 2010, there were an estimated 536 million users of the English language Internet, constituting 27.3\% of all Internet users \cite{stats2}. This makes the English Internet the most used in the world, with only the Chinese Internet coming anywhere close, with 445 million users.  The third most popular language is Spanish, with about 153 million users. 
  
    \boxtext{The English language internet is the most used in the world.}
    
    With 9.1 million registrations in February 2011, the UK's top-level country domain, \textit{.uk} is the fifth most popular extension in the world. It is also the second most used country-specific extension, beaten only by the Germany's \textit{.de} extension\cite{stats3} .
    
    \boxtext{With about 9 million Internet domains, the  \textit{.uk} extension is the world's second most popular country-specific extension. }
    
      The growing importance of the Internet is critical for language technology in two ways. On the one hand, the large amount of digitally available language data represents a rich source for analysing the usage of natural language, in particular by collecting statistical information. On the other hand, the Internet offers a wide range of application areas for which language technology is applicable.
  The most commonly used web application is web search, which involves the automatic processing of language on multiple levels, as we will see in more detail in the second part of this paper. It involves sophisticated language technology, differing for each language. For English, this may consist of matching spelling variations (e.g. British/American variations such as \textit{colour/color}), or using context to distinguish whether the word \textit{fly} refers to a noun (insect) or verb.  
  
  It is an expressed political aim in the UK and other European countries to ensure equal opportunities for everyone. In particular, the \textit{Disability Discrimination Act}, which came into force in 1995, together with the more recent \textit{Equality Act} of 2010, have made it a legal requirement for companies and organisations to ensure that their services and information are accessible to all. This requirement applies directly to websites and Internet services. User-friendly language technology tools offer the principal solution to satisfy this legal regulation, for example by offering speech synthesis for the blind.
  
   \boxtext{The UK's Equality Act of 2010 makes it a legal requirement for companies and organisations to make their websites and Internet services accessible to the disabled. }
  
  Internet users and providers of web content can also profit from language technology in less obvious ways, e.g., in the automatic translation of web contents from one language into another. Considering the high costs associated with manually translating these contents, it may be surprising how little usable language technology is built-in compared to the anticipated need.However, it becomes less surprising if we consider the complexity of the English language, which has been partially highlighted above, and the number of technologies involved in typical language technology applications. 
  
  The next section presents an introduction to language technology and its core application areas, together with an evaluation of current language technology support for English.   }

  \ParallelRText{
 }
   
  \ParallelPar
  
      
   
   

  % --------------------------------------------------------------------------
  \section{Language Technology Support for English}

  \ParallelLText{
    Language technologies are software systems designed to handle human language and are therefore often called ``human language technology''. Human language comes in spoken and written forms. While speech is the oldest and, in terms of human evolution, the most natural form of language communication, complex information and most human knowledge is stored and transmitted through the written word. Speech and text technologies process or produce these different forms of language, and they both use dictionaries and rules of grammar and semantics. This means that language technology (LT) links language to various forms of knowledge, independently of the media (speech or text) in which it is expressed. Figure 2 illustrates the LT landscape. 
    
    When we communicate, we combine language with other modes of communication and information media – for example speaking can involve gestures and facial expressions. Digital texts link to pictures and sounds. Movies may contain language in spoken and written form. In other words, speech and text technologies overlap and interact with other technologies that facilitate processing of multi-modal communication and multimedia documents. 
    In this section, we discuss the main application areas of language technology, i.e., language checking, web search, speech technology, and machine translation. This includes applications and basic technologies such as the following:     \begin{itemize}
      \item spelling correction
      \item authoring support
      \item computer-assisted language learning
      \item information retrieval 
      \item information extraction
      \item text summarisation
      \item question answering
      \item speech recognition 
      \item speech synthesis 
    \end{itemize}
    
    Before discussing the above application areas, we provide a short description of the architecture of a typical LT system.   }

  % --------------------------------------------------------------------------
  \ParallelRText{
   }
  \ParallelPar


  \subsection{Application Architectures}
  
  \ParallelLText{
    Software applications for language processing typically consist of several components that mirror different aspects of language. While such applications are typically very complex, Figure 3 shows a highly simplified architecture of a typical text processing system. The first three modules handle the structure and meaning of the text input:
        \begin{enumerate}
      \item Pre-processing: cleans the data, analyses or removes format-ting, detects the input language, replaces don't with do not in English texts, and so on.
      \item Grammatical analysis: finds the verb, its objects, modifiers and other parts of speech as well as detecting the sentence structure.
      \item Semantic analysis: performs disambiguation (i.e., computes the appropriate meaning of words in a given context); resolves anaphora (i.e., determines which pronouns refer to which nouns in the sentence) and substitutes expressions; and represents the meaning of the sentence in a machine-readable way.
    \end{enumerate}
   After analysing the text, task-specific modules can perform other operations, such as automatic summarisation and database look-ups. 
   In the remainder of this section, we firstly introduce the core application areas for language technology, and follow this with a brief overview of the state of LT research and education today, and a description of past and present research programmes. Finally, we present an expert estimate of core LT tools and resources for English, in terms of various dimensions such as availability, maturity and quality. The general state of LT for the English language is summarised in a matrix (figure 8).  The matrix refers to the tools and resources that are emboldened in the main text of this section. LT support for English is also compared to other languages that are part of this series.   }

  \ParallelRText{
  }
  \ParallelPar


  \subsection{Core Application Areas}
  
  \ParallelLText{
   In this section, we focus on the most important LT tools and resources, and provide an overview of LT activities in the UK.  }
  
  \ParallelRText{
     } 
  
  \ParallelPar


  \subsubsection{Language Checking}

  \ParallelLText{
    Anyone who has used a word processor such as Microsoft Word knows that it has a spell checker that highlights spelling mistakes and proposes corrections. The first spelling correction programs compared a list of extracted words against a dictionary of correctly spelled words. Today, these programs are far more sophisticated. Using language-dependent algorithms for \textbf{grammatical analysis}, they detect errors related to morphology (e.g., plural formation) as well as syntax–related errors, such as a missing verb or a conflict of verb-subject agreement (e.g., she *write a letter). However, most spell checkers will not find any errors in the following text:  
    
    \textit{I have a spelling checker,\\
      It came with my PC.\\
      It plane lee marks four my revue\\
      Miss steaks aye can knot sea\cite{zar1}.}\\

  Handling these kinds of errors usually requires an analysis of the context. This type of analysis either needs to draw on language-specific \textbf{grammars} labouriously coded into the software by experts, or on a statistical language model. In the latter case, a model calculates the probability that a particular word will occur in a specific position (e.g., between the words that precede and follow it). For example, \textit{It plainly marks} is a much more probable word sequence than \textit{It plane lee marks}. A statistical language model can be automatically created by using a large amount of (correct) language data, called a \textbf{text corpus}. 
  
    Language checking is not limited to word processors; it is also used in ``authoring support systems'', i.e., software environments in which manuals and other documentation are written to special standards for complex IT, healthcare, engineering and other products. Fearing customer complaints about incorrect use and damage claims resulting from poorly understood instructions, companies are increasingly focussing on the quality of technical documentation while targeting the international market (via translation or localisation) at the same time. As a result, attempts were made to develop a controlled, simplified technical English that would make it easier for native and non-native readers to understand the instructional text. An example is \textit{ASD-STE100} \cite{asd}, originally developed for aircraft maintenance manuals, but suitable for other technical manuals.   This controlled language contains a fixed basic vocabulary of approximately 1000 words, together with rules for simplifying the sentence structures. Examples of these rules include only using approved meanings for words, as specified in the dictionary (to avoid ambiguity), not writing more than 3 nouns together,  always using the active voice in instruction sentences, and ensuring that such sentences do not exceed a maximum length. Following such rules can make documentation easier to translate into other languages and can also improve the quality of results produced by MT software. The specification is maintained and kept up-to-date by the Simplified Technical English Maintenance Group (STEMG), which consists of members in several different European countries. 
    Advances in natural language processing have led to the development of authoring support software, which helps the writer of technical documentation use vocabulary and sentence structures that are consistent with industry rules and (corporate) terminology restrictions. The HyperSTE software\cite{hss} , developed by Tedopres International, is such an example, which is based on the \textit{ASD-STE100 specification}. 
    Besides spell checkers and authoring support, language checking is also important in the field of computer-assisted language learning. Language checking applications also automatically correct search engine queries, as found in Google's \textit{Did you mean… }suggestions. 
  }
  
  \ParallelRText{
   }
  \ParallelPar


  \subsubsection{Web Search}

  \ParallelLText{
    Searching the Web is probably the most widely used language technology application in use today, although it remains largely underdeveloped. The search engine Google, which started in 1998, is nowadays used for almost 93\% of all search queries in the UK\cite{stats4} . Since 2006, the verb \textit{to google} has even had an entry in the Oxford English dictionary. The Google search interface and results page display has not significantly changed since the first version. However, in the current version, Google offers spelling correction for misspelled words and incorporates basic semantic search capabilities that can improve search accuracy by analysing the meaning of terms in a search query context \cite{pc1}.  The Google success story shows that a large volume of data and efficient indexing techniques can deliver satisfactory results using a statistically approach to language processing.  
    For more sophisticated information requests, it is essential to integrate deeper linguistic knowledge to facilitate text interpretation. Experiments using \textbf{lexical resources} such as machine-readable thesauri or ontological language resources (e.g., WordNet) have shown improvements by allowing pages to be found containing synonyms of the entered search term, e.g., the clever search engine \cite{clev} .  For example, if the search term \textit{nuclear power} is entered into this engine, the search will be expanded to locate also those pages containing the terms \textit{atomic power, atomic energy} or \textit{nuclear energy}. Even more loosely related terms may also be used.  
    
    The next generation of search engines will have to include much more sophisticated language technology, particularly to deal with search queries consisting of a question or other sentence type, rather than a list of keywords. For the query, \textit{Give me a list of all companies that were taken over by other companies in the last five years}, a syntactic as well as a semantic analysis is required. The system also needs to provide an index to quickly retrieve relevant documents. A satisfactory answer will require syntactic parsing to analyse the grammatical structure of the sentence and determine that the user wants companies that have been acquired, rather than companies that have acquired other companies. For the expression \textit{last five years}, the system needs to determine the relevant range of years, taking into account the present year. The query then needs to be matched against a huge amount of unstructured data to find the piece or pieces of information that are relevant to the user's request. This process is called ``information retrieval'', and involves searching and ranking relevant documents. To generate a list of companies, the system also needs to recognise that a particular string of words in a document represents a company name, using a process called ``named entity recognition''.
    A more demanding challenge is matching a query in one language with documents in another language. Cross-lingual information retrieval involves automatically translating the query into all possible source languages and then translating the results back into the user's target language. Now that data is increasingly found in non-textual formats, there is a need for services that deliver multimedia information retrieval by searching images, audio files and video data. In the case of audio and video files, a speech recognition module must convert the speech content into text (or into a phonetic representation) that can then be matched against a user query.
    The first search engines for English appeared in 1993, with many having come and gone since those days. Today, apart from Google, the major players are Microsoft's Bing (accounting for approximately 4\% of UK searches) and Yahoo (approximately 2\% of searches in the UK, but also powered by Bing). All other engines account for less than 1\% of searches. Some sites such as Dogpile provide access to meta-search engines, which fetch results from a range of different search engines. Other search engines focus on specialised topics and incorporate semantic search, an example being Yummly, which deals exclusively with recipes. Blinx is an example of a video search engine, which makes use of a combination of conceptual search, speech recognition and video analysis software to locate videos of interest to the user.  }
  
  \ParallelRText{
   
  }
  
  \ParallelPar


  \subsubsection{Speech Interaction}

  \ParallelLText{
    Speech interaction is one of many application areas that depend on speech technology, i.e., technologies for processing spoken language. Speech interaction technology is used to create interfaces that enable users to interact in spoken language instead of using a graphical display, keyboard and mouse. Today, these voice user interfaces (VUI) are used for partially or fully automated telephone services provided by companies to customers, employees or partners. Business domains that rely heavily on VUIs include banking, supply chain, public transportation, and telecommunications. Other uses of speech interaction technology include interfaces to car navigation systems and the use of spoken language as an alternative to the graphical or touch-screen interfaces in smartphones. 
    
Speech interaction comprises four technologies:    
 \begin{enumerate}
      \item Automatic \textbf{speech recognition} (ASR) determines which words are actually spoken in a given sequence of  
      sounds uttered by a user.
      \item Natural language understanding analyses the syntactic structure of a user‚Äôs utterance and interprets it 
      according to the system in question.
      \item Dialogue management determines which action to take given the user input and system functionality.    
      \item \textbf{Speech synthesis} (text-to-speech or TTS) transforms the system‚Äôs reply into sounds for the user.
    \end{enumerate}
    
    
    One of the major challenges of ASR systems is to accurately recognise the words a user utters. This means restricting the range of possible user utterances to a limited set of keywords, or manually creating language models that cover a large range of natural language utterances. Using machine learning techniques, language models can also be generated automatically from \textbf{speech corpora}, i.e., large collections of speech audio files and text transcriptions. Restricting utterances usually forces people to use the voice user interface in a rigid way and can damage user acceptance; but the creation, tuning and maintenance of rich language models will significantly increase costs. VUIs that employ language models and initially allow a user to express their intent more flexibly -prompted by a \textit{How may I help you?} greeting - tend to be automated and are better accepted by users. 
    
    Companies tend to use utterances pre-recorded by professional speakers for generating the output of the voice user interface. For static utterances, where the wording does not depend on particular contexts of use or personal user data, this can deliver a rich user experience. However, more dynamic utterance content may suffer from unnatural intonation, because parts of audio files have simply been strung together. Through optimisation, Today's TTS systems are getting better at producing natural-sounding dynamic utterances.  
    
    Speech interaction interfaces have been considerably standardised during the last decade in terms of their various technological components. There has also been strong market consolidation in speech recognition and speech synthesis. The national markets in the G20 countries (economically resilient countries with high populations) have been dominated by just five global players, with Nuance (USA) and Loquendo (Italy) being the most prominent players in Europe. In 2011, Nuance announced the acquisition of Loquendo, which represents a further step in market consolidation.
    On the UK TTS market, Google's interest in TTS technology has been demonstrated by their recent acquisition of Phonetic Arts \cite{PhonArts} , a company that already counted global giants such as Sony and EA Games amongst its clients. One of the selling points of Edinburgh-based CereProc is the provision of voices that have character and emotion. Roktalk is a screen reader to enhance accessibility of web-sites, whilst Ocean Blue Software, a digital television software provider, has recently developed a low-cost text-to-speech technology called 'Talk TV', which has the aim of making the viewing of TV more accessible to those with visual impairment. The technology has been used to create the world's first accessible technology solution designed to provide speech/talk-based TV programming guides and set up menus.  The Festival Speech Synthesis System \cite{festival}  is free software that has been actively under development for several years by the University of Edinburgh, with both British and American voices, in addition to Spanish and Welsh capabilities. 
    
    Regarding dialogue management technology and know-how, markets are strongly dominated by national players, which are usually SMEs. Today's key players in the UK include Vicorp and Sabio. Rather than exclusively relying on a product business based on software licenses, these companies have positioned themselves mostly as full-service providers that offer the creation of VUIs as a system integration service. In the area of speech interaction, there is as yet no real mar get for syntactic and semantic analysis-based core technologies.   
    Looking ahead, there will be significant changes, due to the spread of smartphones as a new platform for managing customer relation-ships, in addition to fixed telephones, the Internet and e-mail. This will also affect how speech technology is used. In the long term, there will be fewer telephone-based VUIs, and spoken language will play a far more central role as a user-friendly input for smartphones. This will be largely driven by stepwise improvements in the accuracy of speaker-independent speech recognition via the speech dictation services already offered as centralised services to smartphone users. 
         }

  \ParallelRText{
  }
  
  \ParallelPar


  \subsubsection{Machine Translation}
  
  \ParallelLText{
  The idea of using digital computers to translate natural languages can be traced back to 1946 and was followed by substantial funding for research during the 1950s and again in the 1980s. Yet \textbf{machine translation} (MT) still cannot meet its initial promise of across-the-board automated translation.  
  The most basic approach to machine translation is the automatic replacement of words in a text written in one natural language with the equivalent words of another language. This can be useful in subject domains that have a very restricted, formulaic language, such as weather reports. However, in order to produce a good translation of less restricted texts, larger text units (phrases, sentences, or even whole passages) need to be matched to their closest counterparts in the target language. The major difficulty is that human language is ambiguous. Ambiguity creates challenges on multiple levels, such as word sense disambiguation on the lexical level (a \textit{jaguar} is a brand of car or an animal) or the attachment of prepositional phrases on the syntactic level as in:
  \textit{The policeman observed the man with the telescope.}\\\textit{The policeman observed the man with the revolver.}\\
  One way to build an MT system is to use linguistic rules. For translations between closely related languages, a translation using direct substitution may be feasible in cases such as the above. However, rule-based (or linguistic knowledge-driven) systems often analyse the input text and create an intermediary symbolic representation from which the target language text can be generated. The success of these methods is highly dependent on the availability of extensive lexicons with morphological, syntactic, and semantic information, and large sets of grammar rules carefully designed by skilled linguists. This is a very long and therefore costly process.
  In the late 1980s, when computational power increased and became cheaper, interest in statistical models for machine translation began to grow. Statistical models are derived from analysing bilingual text corpora (\textbf{parallel corpora}), such as the Europarl parallel corpus, which contains the proceedings of the European Parliament in 11 European languages. Given enough data, statistical MT works well enough to derive an approximate meaning of a foreign language text by processing parallel versions and finding plausible patterns of words. Unlike knowledge-driven systems, however, statistical (or data-driven) MT systems often generate ungrammatical output. Data-driven MT is advantageous because less human effort is required, and it can also cover special particularities of the language (e.g., idiomatic expressions) that are often ignored in knowledge-driven systems. 
  The strengths and weaknesses of knowledge-driven and data-driven machine translation tend to be complementary, so that nowadays researchers focus on hybrid approaches that combine both methodologies. One such approach uses both knowledge-driven and data-driven systems, together with a selection module that decides on the best output for each sentence. However, results for sentences longer than, say, 12 words, will often be far from perfect. A more effective solution is to combine the best parts of each sentence from multiple outputs; this can be fairly complex, as corresponding parts of multiple alternatives are not always obvious and need to be aligned. 
  There are several research groups in the UK and the USA active in machine translation, both in academia and industry. These include the Natural Language and Information Processing Group of the University of Cambridge, the Statistical Machine Translation Group of the University of Edinburgh, the Center for Machine Translation at the Carnegie Mellon University and the Natural Language Processing groups at both Microsoft Research and IBM Research. 
  SYSTRAN is one of the oldest machine translation companies, founded in 1968 in the USA and having carried out extensive work for the United States Department of Defense and the European Commission. The current version uses hybrid technology and offers capabilities to translate between 52 different languages. SYSTRAN is used to provide translation services on the Internet portals Yahoo, Lycos and AltaVista. Although Google originally also made use of SYSTRAN's services, they now use their own statistical-based system, which supports 57 different languages. Microsoft uses their own syntax-based statistical machine translation technology to provide translation services within their Bing search engine. 
  In the UK, automated translation solutions are provided by companies such as SDL, who provide a free web-based translation service in addition to commercial products.  Very specialised MT systems have also been developed, e.g., the LinguaNet system, created by Cambridge-based Prolingua. This is a specially designed messaging system for cross border, mission critical operational communication by police, fire, ambulance, medical, coastguard, disaster response coordinators. It is currently used by 50 police sites in Belgium, France, the Netherlands, Spain, United Kingdom, Den-mark, and Germany. 
  There is still a huge potential for improving the quality of MT systems. The challenges involve adapting language resources to a given subject domain or user area, and integrating the technology into workflows that already have term bases and translation memories.   
  Evaluation campaigns help to compare the quality of MT systems, the different approaches and the status of the systems for different language pairs. The table below, which was prepared during the EC Euromatrix+ project, shows the pair-wise performances obtained for 22 of the 23 official EU languages (Irish was not compared). The results are ranked according to a BLEU score, which indicates higher scores for better translations \cite{bleu1}.   A human translator would normally achieve a score of around 80 points.
  The best results (in green and blue) were achieved by languages that benefit from a considerable research effort in coordinated programs and from the existence of many parallel corpora (e.g., English, French, Dutch, Spanish and German). The languages with poorer results are shown in red. These languages either lack such development efforts or are structurally very different from other languages (e.g., Hungarian, Maltese and Finnish).
  
  Performance of Machine Translation for Language Pairs in the Euromatrix+ Project.
  }

  \ParallelRText{
        
  }
  
  \ParallelPar


  \subsection{Other Application Areas}

  \ParallelLText{
     Building language technology applications involves a range of subtasks that do not always surface at the level of interaction with the user, but they provide significant service functionalities ``behind the scenes'' of the system in question. They all form important research issues that have now evolved into individual sub-disciplines of computational linguistics. 

    Question answering, for example, is an active area of research for which annotated corpora have been built and scientific competitions have been initiated. The concept of question answering goes beyond keyword-based searches (in which the search engine responds by delivering a collection of potentially relevant documents) and enables users to ask a concrete question to which the system provides a single answer. For example:
    
    
    \textit{Question: How old was Neil Armstrong when he stepped on the moon?\\
    Answer: 38.}\\

    Question answering is in turn related to information extraction (IE), an area that was extremely popular and influential when computational linguistics took a statistical turn in the early 1990s. IE aims to identify specific pieces of information in specific classes of documents, such as the key players in company takeovers, as reported in newspaper stories. Another common scenario that has been studied is reports on terrorist incidents. The task here consists of mapping appropriate parts of the text to a template that specifies the perpetrator, target, time, location and results of the incident. Domain-specific template-filling is the central characteristic of IE, which makes it another example of a ``behind the scenes'' technology that forms a well-demarcated research area, which in practice needs to be embedded into a suitable application environment. 
    

    Text summarisation and \textbf{text generation} are two borderline areas that can act either as standalone applications or play a supporting role. Summarisation attempts to give the essentials of a long text in a short form, and is one of the features available in Microsoft Word. It mostly uses a statistical approach to identify the ‚Äúimportant‚Äù words in a text (i.e., words that occur very frequently in the text in question but less frequently in general language use) and determine which sentences contain the most of these important words. These sentences are then extracted and put together to create the summary. In this very common commercial scenario, summarisation is simply a form of sentence extraction, and the text is reduced to a subset of its sentences. An alternative approach, for which some research has been carried out, is to generate brand new sentences that do not exist in the source text. This requires a deeper understanding of the text, which means that so far this approach is far less robust. On the whole, a text generator is rarely used as a stand-alone application but is embedded into a larger software environment, such as a clinical information system that collects, stores and processes patient data. Creating reports is just one of many applications for text summarisation. 

   For English, question answering, information extraction, and summarisation have been the subject of numerous open competitions since the 1990s, primarily organised by DARPA/NIST in the United States, which have significantly improved the state of the art.  For example, the annual TREC (Text REtrieval Conference) series included a question-answering track between 1999 and 2007. Recently, freely accessible tools have been developed that reason and compute the answers. These include True Knowledge, developed in the UK, and Wolfram Alpha, developed in the USA. Question-answering systems in more specialist domains have also begun to emerge, such as the EAGLi system for questions answer-ing in the Genomics literature, developed at the University of Applied Sciences, Geneva. 
   
Information Extraction research was boosted by both the series of MUCs (Message Understanding Conferences), running from 1987-1998, and subsequently by the Automatic Content Extraction (ACE) program, running from 1999 to 2008. Domain-specific challenges such as BioCreAtIvE (Critical Assessment of Information Extraction systems in Biology), of which the most recent was held in 2010, have helped to further research into Information Extraction from more specialised  types of text. Evaluation of text summarisation systems was carried out as part of the Document Understanding Conferences (DUC) from 2001-2007, and more recently as one of the tracks in the Text Analysis Conferences (TAC).  Web-based tools such as Ultimate Research Assistant and iRe-search Reporter can produce summary reports of retrieved search results.   }

  \ParallelRText{
   
  }
  
  \ParallelPar


  \subsection{Educational Programmes}

  \ParallelLText{
    In the UK, a large number of universities have well-established research groups that are active in the field of language technology or computational linguistics. These are complemented by many other groups in English speaking countries, most notably the USA, Australia and Ireland. These groups are most often part of either computer science or linguistics departments. The University of Manchester hosts the National Centre for Text Mining (NaCTeM), which is the world's first publicly funded text mining centre, providing text mining services to both academic institutions and industrial organisations. Over the past few years, there has been an increasing interest in tools and resources dealing with specialist domains such as biomedicine, molecular biology and chemistry. 
    In terms of teaching in the UK, courses with a large element of natural language processing or computational linguistics are rare, and are normally only offered at the masters level. Examples include the MSc in Speech and Language Processing and the MSc in Cognitive Science, offered at the University of Edinburgh. A greater number of universities offer course modules in NLP to students of more general degree programs. Examples include Birmingham, Cambridge, Manchester and Leeds. 
}
  \ParallelRText{
      }
  
  \ParallelPar


  \subsection{National Projects and Efforts}

  \ParallelLText{
    The first working demonstration of an LT system took place in the 1950s. This system constituted a Russian – English Machine Translation (MT) system, developed by IBM and Georgetown University.   The company SYSTRAN, which was founded in 1968, had the original aim of processing the same language pair for the Unit-ed States Airforce. SYSTRAN still exists today, as described in the \textit{Machine translation} section above. 
    An early LT program, EUROTRA, was an ambitious Machine Translation (MT) project inspired by the modest success of SYSTRAN, and established and funded by the European Commission from the late 1970s until 1994. The project was motivated by one of the founding principles of the EU: that all citizens had the right to read any and all proceedings of the Commission in their own language. A large network of European computational linguists embarked upon the EUROTRA project with the hope of creating a state-of-the-art MT system for the then seven, later nine, official languages of the European Community. However, as time passed, expectations became tempered; "Fully Automatic High Quality Translation" was not a reasonably attainable goal. The true character of EUTOTRA was eventually acknowledged to be in fact pre-competitive research rather than prototype development. While EUROTRA never delivered a "working" MT system, the project made a far-reaching long-term impact on the nascent language industries in European member states. 
    The Alvey Programme was the dominating focus of Information Technology research in the UK between 1983 and 1988. Amongst the areas of focus was Man Machine Interaction. The programme funded three projects at the Universities of Cambridge, Edinburgh and Lancaster to provide tools for use in natural language processing research. The tools, i.e., a morphological analyser, parsers, a grammar and lexicon were usable individually as well as together - integrated by a grammar development environment - forming a complete system for the morphological, syntactic and semantic analysis of a considerable subset of English.
    The creation of the British National Corpus (BNC) was a major project that took place between 1991 and 1994. The corpus constitutes a 100 million word collection of samples of written and spoken language from a wide range of sources, designed to represent a wide cross-section of British English from the later part of the 20th century. The corpus is encoded according to the Guidelines of the Text Encoding Initiative (TEI) to represent both the output from CLAWS (automatic part-of-speech tagger) and a variety of other structural properties of texts (e.g. headings, paragraphs, lists etc.). An XML version of the corpus was released in 2007. 
    Corpora of other varieties of English are also being collected. The International Corpus of English (ICE), whose collection began in 1990, involves 23 research teams around the world, who are pre-paring electronic corpora of their own national or regional variety of English. Each team is producing a corpus consisting of one million words of spoken and written English produced after 1989. The Corpus of Contemporary American English (COCA) consists of 425 million words, equally divided among spoken, fiction, popular magazines, newspapers, and academic texts, consisting of 20 million words each year from 1990-2011. 
    AKT (2000-2007), was a multi-million pound collaboration between 5 UK universities, with the aim of enhancing information and knowledge management in the age of the World Wide Web. The team of 119 staff was interdisciplinary, involving leading figures in the worlds of multimedia, natural language and computational linguistics, agents, artificial intelligence, formal methods, machine learning and e-science. The research conducted on the project formed an important contribution to the semantic web, in which the use of LT technologies played a central role.  The AKT collaboration was a significant success in terms of papers published, grants awarded (36 other projects), students trained and international impact. It was rated as ``outstanding'' by the review panel.  The collaboration placed major importance on making links with industrial partners, and finally it led to the founding of a number of spin-off companies. A follow-up project, EnAKTing the Unbounded Data Web: Challenges in Web Science, is currently ongoing.  
    Since many LT applications make use of similar sets of processing components, such as tokenizers, taggers, parsers, named entity recognisers, etc., the speed with which new applications can be developed can be greatly increased if such processing components can be reused and repurposed in flexible ways to create a range of different LT applications. Two systems which support the user in creating new applications from existing libraries of processing components are the University of Sheffield's GATE system, which has been under development for over 15 years, and the more recent U-Compare system, which was developed as part of a collaboration between the Universities of Tokyo, Manchester and Colorado. Whilst current components in U-Compare mainly deal with English, the library will be extended as part of META-NET to cover a number of different European languages. 
    As we have seen, previous programmes have led to the development of a large number of LT tools and resources for the English language. In the following section, the current state of LT support for English is summarised.  }

  \ParallelRText{
    }

  \ParallelPar


  \subsection{ Availability of Tools and Resources}

  
  \ParallelLText{
   Figure 8 provides a general picture of the current state of language technology support for the English language. This rating of existing tools and resources was generated by leading experts in the filed, who provided estimates based on scale from 0 (very low) to 6 (very high) according to seven different criteria.
      
   \ParallelRText{}
    % Table here

    For English, key results regarding technologies and resources include the following:
    \begin{itemize}
       \item No single category of technology or resources has consistently high scores across all criteria being evaluated.         
      \item Generally, quantity, quality and availability can only be guaranteed for tools and resources dealing with more basic levels of linguistic processing.    
      \item  Higher levels of linguistic processing still present considerable challenges. The lower number of corpora annotated with these levels of information could be a factor limiting the advancement of these technologies, since the development of such technologies is more difficult if the amount of data on which they can be trained is limited.
          \item In general, speech processing technology is better developed than text processing technology. Indeed, speech technology has already been integrated into many everyday applications, from spoken dialogue systems and voice-based interfaces to mobile phones and in-car satellite navigation systems.
      \item Sustainability is, in general, a major area of concern. Even if high quality technologies and resources exist, major efforts may still be required to ensure that they are kept up-to-date and can easily be integrated into other systems. There is also often a lack of rigorous software testing/engineering principles applied to tools. The availability of the high-performance Lucene search engine for Information Retrieval, and the high quality test suites for grammar engineering, make these two areas notable exceptions.  
      \item In general, tools that work well on a particular type of text may require considerable work to allow them to be applied to new text domains. Resources such as annotated corpora are also normally domain-specific, and creating such corpora for new domains generally requires a large amount of manual work.      
      \item  For all technologies and tools, there are examples that are available free of charge. However, the number of such tools and re-sources varies greatly according to category.  In some cases, quality comes at a price. For example, in the case of syntactic corpora, there is little to rival the Penn TreeBank, which is only available for a fee. In other cases, even large corpora are available free of charge, e.g. Google's n-gram corpus for statistical language modelling, which was created from 1 trillion word tokens of text from publicly accessible Web pages.       
      \item Some broad areas, such as semantic analysis, consist of a number of component technologies. Whilst some of these technologies (e.g. named entity tagging), are quite mature and can produce high quality results, others, such as event/relation ex-traction are more complex and still require improvement.  The scores awarded attempt to balance the different stages of development of these technologies.     
       \item The current legal situation restricts making use of digital texts for empirical linguistic and language technology research, for example, to train statistical language models.  However, a recent review by Professor Ian Hargreaves represents a step forward towards an Intellectual Property regime which is suited to the needs of 21st century business and consumers. Implementation of the proposals would allow copyrighted texts texts that have been legally acquired/bought/subscribed to, to used by researchers for language-related R\&D activities. The UK Government has accepted the proposals and is currently consulting on implementation     
        \item The cooperation between the Language Technology community and those involved with the Semantic Web and the closely related Linked Open Data movement should be intensified with the goal of establishing a collaboratively maintained, machine-readable knowledge base that can be used both in web-based information systems and as semantic knowledge bases in LT applications ‚Äì ideally, this endeavour should be addressed in a multilingual way on the European scale.
    \end{itemize}
    
  }
  
  \ParallelPar


  \subsection{Cross-language comparison}

 
  \ParallelLText{
    The current state of LT support varies considerably from one language community to another. In order to compare the situation between languages, this section will present an evaluation based on two sample application areas (machine translation and speech processing) and one underlying technology (text analysis), as well as basic resources needed for building LT applications.  The languages were categorised using the following five-point scale:

  


  \begin{enumerate}
\item Excellent support
\item Good support
\item Moderate support
\item Fragmentary support
\item Weak or no support
\end{enumerate}

LT support was measured according to the following criteria:

\textbf{Speech Processing:} Quality of existing speech recognition technologies, quality of existing speech synthesis technologies, coverage of domains, number and size of existing speech corpora, amount and variety of available speech-based applications.

\textbf{Machine Translation:} Quality of existing MT technologies, number of language pairs covered, coverage of linguistic phenomena and domains, quality and size of existing parallel corpora, amount and variety of available MT applications.

\textbf{Text Analysis:} Quality and coverage of existing text analysis technologies (morphology, syntax, semantics), coverage of linguistic phenomena and domains, amount and variety of available applications, quality and size of existing (annotated) text corpora, quality and coverage of existing lexical resources (e.\,g., WordNet) and grammars.

\textbf{Resources:} Quality and size of existing text corpora, speech corpora and parallel corpora, quality and coverage of existing lexical resources and grammars.


The above tables show that, thanks to large-scale LT funding in recent decades, the English language is generally one of the best-equipped languages. However, as can be seen from the above clusters, there is not a single area in which resources for English can be classified as having excellent support. Thus, there is are many gaps to be filled with regards to high quality applications for English.
   For speech processing, current technologies perform well enough to be successfully integrated into a number of industrial applications such as spoken dialogue and dictation systems. Today's text analysis components and language resources already cover the linguistic phenomena of English to a certain extent and form part of many applications involving mostly shallow natural language processing, e.g. spelling correction and authoring support.
   However, for building more sophisticated applications, such as machine translation, there is a clear need for resources and technologies that cover a wider range of linguistic aspects and allow a deep semantic analysis of the input text. By improving the quality and coverage of these basic resources and technologies, we shall be able to open up new opportunities for tackling a vast range of advanced application areas, including high-quality machine translation. 
  }
   \ParallelRText{}
   
  \ParallelPar


  \subsection{Conclusions}

  
  \ParallelLText{
    \boxtext{In this series of white papers, we have made an important initial effort to assess language technology support for 30 European languages, and provide a high-level comparison across these languages. By identifying the gaps, needs and deficits, the European language technology community and related stakeholders are now in a position to design a large scale research and development programme aimed at building a truly multilingual, technology-enabled Europe.}

    We have seen that there are huge differences between Europe's languages. While there are good quality software and resources available for some languages and application areas, others (usually ‘smaller' languages) have substantial gaps. Many languages lack basic technologies for text analysis and the essential resources for developing these technologies. Others have basic tools and re-sources but are as yet unable to invest in semantic processing. We therefore still need to make a large-scale effort to attain the ambitious goal of providing high-quality machine translation between all European languages.  
    It is without doubt that there exist extremely strong foundations on which the already thriving language technology landscape for English can continue to grow and prosper, especially given the well established research communities both in the UK and other English-speaking countries worldwide. However, it is important to emphasise that many aspects of language technology have still yet to be solved. In certain cases, some of these problems concern the need to focus greater research efforts on some of the more complex areas of LT, including advanced discourse processing and language generation. However, some more general issues, including problems of sustainability and adaptability, which are common across many types of tools and resources, are in urgent need of more focussed strategies.The English language technology industry dedicated to transform-ing research into products is currently fragmented and disorganised. Most large companies have either stopped or severely cut their LT efforts, leaving the field to a number of specialised SMEs that are not robust enough to address the internal and the global market with a sustained strategy. 
    Our findings show that the only alternative is to make a substantial effort to create LT resources for English, and use them to drive forward research, innovation and development. The need for large amounts of data and the extreme complexity of language technology systems makes it vital to develop a new infrastructure and a more coherent research organisation to spur greater sharing and cooperation.
    The long term goal of META-NET is to enable the creation of high-quality language technology for all languages. This requires all stakeholders - in politics, research, business, and society - to unite their efforts. The resulting technology will help tear down existing barriers and build bridges between Europe‚Äôs languages, paving the way for political and economic unity through cultural diversity. 
  }
  
  \ParallelRText{}

  \ParallelPar


  % --------------------------------------------------------------------------
  \section{About META-NET}
  
 
  \ParallelLText{
    META-NET is a Network of Excellence funded by the European Commission. The network currently consists of 47 members from 31 European countries. META-NET fosters the Multilingual Europe Technology Alliance (META), a growing community of language technology professionals and organisations in Europe. 

    META-NET cooperates with other initiatives like the Common Language Resources and Technology Infrastructure (CLARIN), which is helping establish digital humanities research in Europe. META-NET fosters the technological foundations for a truly multilingual European information society that:
    \begin{itemize}
      \item makes communication and cooperation possible across languages;
      \item provides equal access to information and knowledge in any language;
      \item offers advanced and affordable networked information technology to European citizens.
    \end{itemize}
    META-NET stimulates and promotes multilingual technologies for all European languages. The technologies enable automatic translation, content production, information processing and knowledge management for a wide variety of applications and subject domains. The network wants to improve current approaches, so better communication and cooperation across languages can take place. Europeans have an equal right to information and knowledge regardless of language. 
  }
  
    \ParallelRText{}  

  \ParallelPar


  \subsection{Lines of Action}

 
  \ParallelLText{
    META-NET launched on 1 February 2010 with the goal of advancing research in language technology (LT). The network supports a Europe that unites as a single digital market and information space. META-NET has conducted several activities that further its goals. META-VISION, META-SHARE and META-RESEARCH are the network‚Äôs three lines of action.

    Three Lines of Action in META-NET

    \textbf{META-VISION} fosters a dynamic and influential stakeholder community that unites around a shared vision and a common strategic research agenda (SRA). The main focus of this activity is to build a coherent and cohesive LT community in Europe by bringing together representatives from highly fragmented and diverse groups of stakeholders. In the first year of META-NET, presentations at the FLaReNet Forum (Spain), Language Technology Days (Luxembourg), JIAMCATT 2010 (Luxembourg), LREC 2010 (Malta), EAMT 2010 (France) and ICT 2010 (Belgium) centred on public outreach. According to initial estimates, META-NET has already contacted more than 2,500 LT professionals to develop its goals and visions with them. At the META-FORUM 2010 event in Brussels, META-NET communicated the initial results of its vision building process to more than 250 participants. In a series of interactive sessions, the participants provided feedback on the visions presented by the network. 

    \textbf{META-SHARE} creates an open, distributed facility for exchanging and sharing resources. The peer-to-peer network of repositories will contain language data, tools and web services that are documented with high-quality metadata and organised in standardised categories. The resources can be readily accessed and uniformly searched. The available resources include free, open source materials as well as restricted, commercially available, fee-based items. META-SHARE targets existing language data, tools and systems as well as new and emerging products that are required for building and evaluating new technologies, products and services. The reuse, combination, repurposing and re-engineering of language data and tools plays a crucial role. META-SHARE will eventually become a critical part of the LT marketplace for developers, localisation experts, researchers, translators and language professionals from small, mid-sized and large enterprises. META-SHARE addresses the full development cycle of LT‚Äîfrom research to innovative products and services. A key aspect of this activity is establishing META-SHARE as an important and valuable part of a European and global infrastructure for the LT community. 

    \textbf{META-RESEARCH} builds bridges to related technology fields. This activity seeks to leverage advances in other fields and to capitalise on innovative research that can benefit language technology. In particular, this activity wants to bring more semantics into machine translation (MT), optimise the division of labour in hybrid MT, exploit context when computing automatic translations and prepare an empirical base for MT. META-RESEARCH is working with other fields and disciplines, such as machine learning and the Semantic Web community. META-RESEARCH focuses on collecting data, preparing data sets and organising language resources for evaluation purposes; compiling inventories of tools and methods; and organising workshops and training events for members of the community. This activity has already clearly identified aspects of MT where semantics can impact current best practices. In addition, the activity has created recommendations on how to approach the problem of integrating semantic information in MT. META-RESEARCH is also finalising a new language resource for MT, the Annotated Hybrid Sample MT Corpus, which provides data for English-German, English-Spanish and English-Czech language pairs. META-RESEARCH has also developed software that collects multilingual corpora that are hidden on the Web.
  }
  
