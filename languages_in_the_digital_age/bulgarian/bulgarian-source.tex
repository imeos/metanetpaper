\ssection[Language Technology Support for Bulgarian]{Language Technology Support\newline for Bulgarian}

\begin{multicols}{2}

Language technologies are software systems designed to handle human language and are therefore often called "human language technology". Human language comes in spoken and written forms. While speech is the oldest and in terms of human evolution the most natural form of language communication, complex information and most human knowledge is stored and transmitted in written texts. Speech and text technologies process or produce these different forms of language, though they both use dictionaries and rules of grammar and semantics. This means that language technology (LT) links language to various forms of knowledge, independently of the media (speech or text) it is expressed in. The figure illustrates the LT landscape. When we communicate, we combine language with other modes of communication and information media – for example speaking can involve gestures and facial expressions. Digital texts link to pictures and sounds. Movies may contain language in spoken and written f
 orm. In other words, speech and text technologies overlap and interact with other technologies that facilitate processing of multimodal communication and multimedia documents. 

In the following, we will discuss the main application areas of language technology, i.e., language checking, web search, speech technology, and machine translation. This includes applications and basic technologies such as 
\begin{itemize}
\item spelling correction;
\item authoring support;
\item computer-assisted language learning;
\item information retrieval;
\item information extraction;
\item text summarisation;
\item question answering;
\item speech recognition; 
\item speech synthesis.
\end{itemize}

Before discussing the above application areas, we will shortly describe the architecture of a typical LT system. 

\subsection{Application Architectures}

Software applications for language processing typically consist of several components that mirror different aspects of language. The figure shows a highly simplified architecture that can be found in a typical text processing system. The first three modules handle the structure and meaning of the text input:
\begin{enumerate}
\item Pre-processing: cleans the data, analyses or removes formatting, detects the input language, and so on.
\item Grammatical analysis: finds the verb, its objects, modifiers and other parts of speech as well as detects the sentence structure.
\item Semantic analysis: performs disambiguation (i.e., computes the appropriate meaning of words in a given context); resolves anaphora (i.e., which pronouns refer to which nouns in the sentence) and substitute expressions; and represents the meaning of the sentence in a machine-readable way.
\end{enumerate}

After analysing the text, task-specific modules can perform other operations, such as automatic summarisation and database look-ups. This is a simplified and idealised description of the application architecture and illustrates the complexity of LT applications. 

After introducing the core application areas for language technology, we shall provide a brief overview of the state of LT research and education today, and end with an overview of past and present research programmes. We shall then present an expert estimate of core LT tools and resources in terms of various dimensions such as availability, maturity and quality. The general situation of LT for the Bulgarian language is summarised in a table.

\subsection{Core Application Areas}

In this section, we focus on the most important LT tools and resources, and give an overview of LT activities in Bulgaria. Tools and resources that are \uline{underlined} in the text can also be found in the table at the end of this chapter.  

\subsubsection{Language checking}

Anyone who has used a word processor such as Microsoft Word knows that it has a spelling checker that highlights spelling mistakes and proposes corrections. The first spelling correction programs compared a list of extracted words against a dictionary of correctly spelled words. Today these programs are far more sophisticated. Using language-dependent algorithms for \uline{text analysis}, they detect errors related to morphology (e.g., plural formation) as well as syntax–related errors, such as a missing verb or a conflict of verb-subject agreement (e.g., \textit{Tya *napisahme pismoto} [she *write a letter]). But most spell checkers will not find any errors in the following text:  

{\it
\hspace{0.5cm}Eye have a spelling chequer,

\hspace{0.5cm}It came with my Pea Sea.

\hspace{0.5cm}It plane lee marks four my revue

\hspace{0.5cm}Miss Steaks I can knot sea. \cite{zar1}
}

For handling this type of error, \uline{context} analysis is needed in many cases, e.g., to decide if a word needs to be written in upper case, as in:

{\it
\hspace{0.5cm}{Tya zhivee v \textbf{Stara} Zagora.}
}

\hspace{0.5cm}[She lives in Stara Zagora.]

{\it
\hspace{0.5cm}{Tya e \textbf{stara} zhena.}
}

\hspace{0.5cm}[She is an old woman.]


This type of analysis either needs to draw on language-specific \uline{grammars} laboriously coded into the software by experts, or on a statistical language model. In this case, a model calculates the probability of a particular word as it occurs in a specific position (e.g., between the words that precede and follow it). A statistical language model can be automatically created by using a large amount of (correct) language data (called a \uline{text corpus}). Most of these two approaches have been developed around data from English. Neither approach can transfer easily to Bulgarian because the language has a flexible word orderand a richer inflection system. 

Language checking is not limited to word processors; it is also used in "authoring support systems", i.e. software environments in which manuals and other documentation are written to special standards for complex IT, healthcare, engineering and other products. Fearing customer complaints about incorrect use and damage claims resulting from poorly understood instructions, companies are increasingly focusing on the quality of technical documentation while targeting the international market (via translation or localisation) at the same time. Advances in natural language processing have led to the development of authoring support software, which helps the writer of technical documentation use vocabulary and sentence structures that are consistent with industry rules and (corporate) terminology restrictions.

Besides spell checkers and authoring support, language checking is also important in the field of computer-assisted language learning. And language checking applications also automatically correct search engine queries, as found in Google's \textit{Did you mean...} suggestions. 

\subsubsection{Web search}

Searching the Web, intranets or digital libraries is probably the most widely used yet largely underdeveloped language technology application today. The Google search engine, which started in 1998, now handles about 80\% of all search queries \cite{spi1}. Since 2004, the verb googeln even has an entry in the Duden dictionary. The Google search interface and results page display has not significantly changed since the first version. Yet in the current version, Google offers spelling correction for misspelled words and  has now incorporated basic semantic search capabilities that can improve search accuracy by analysing the meaning of terms in a search query context  \cite{pc1}. The Google success story shows that a large volume of available data and efficient indexing techniques can deliver satisfactory results for a statistically-based approach.  

For more sophisticated information requests, it is essential to integrate deeper linguistic knowledge for \uline{text interpretation}. Experiments using \uline{lexical resources} such as machine-readable thesauri or ontological language resources (e.g., WordNet for English or BulNet for Bulgarian) have demonstrated improvements in finding pages using synonyms of the original search terms, such as {\it atomna energiya} [atomic energy] and {\it yadrena energiya} [nuclear energy], or even more loosely related terms. 

The next generation of search engines will have to include much more sophisticated language technology, in particular in order to deal with search queries consisting of a question or other sentence type rather than a list of keywords. For the query, \textit{"Give me a list of all companies that were taken over by other companies in the last five years"}, the LT system needs to analyse the sentence syntactically and semantically as well as provide an index to quickly retrieve relevant documents. A satisfactory answer will require syntactic parsing to analyse the grammatical structure of the sentence and determine that the user wants companies that have been acquired, not companies that acquired other companies. For the expression \textit{last five years}, the system needs to determine the relevant years. And, the query needs to be matched against a huge amount of unstructured data to find the piece or pieces of relevant information the user wants. This is called "information r
 etrieval", and involves searching and ranking relevant documents. To generate a list of companies, the system also needs to recognise a particular string of words in a document as a company name, a process called "named entity recognition".

A more demanding challenge is matching a query in one language with documents in another language. Cross-lingual information retrieval involves automatically translating the query into all possible source languages and then translating the results back into the target language. 

Now that data is increasingly found in non-textual formats, there is a need for services that deliver multimedia information retrieval by searching images, audio files and video data. In the case of audio and video files, a speech recognition module must convert the speech content into text (or into a phonetic representation) that can then be matched against a user query.

Certain Bulgarian portals have crawler software similar to those used by global search engines designed to index sites included within its categories. For example, Dir.bg, one of the first and largest web portals in Bulgaria launched a standalone service – Diri.bg. Diri (in Bulgarian {дири}) is an old word for ‘search’. 


Open source based technologies like Lucene and SOL are often used by search-focused companies to provide the basic search infrastructure. Other search-based companies rely on international search technologies like, e.g., FAST or Exalead. Developmental focus for these companies lies in providing add-ons and advanced search engines for special-interest portals by exploiting topic-relevant semantics. Due to the still high demands in processing power, such search engines are only economically usable on relatively small text corpora. Processing time easily exceeds that of a common statistical search engine as, e.g., provided by Google by a magnitude of thousands. These search engines also have high demand in topic-specific domain modelling, making it not feasible to use these mechanisms on web scale.

\subsubsection{Speech Technology}

Speech interaction technology is used to create interfaces that enable users to interact in spoken language instead of a graphical display, keyboard and mouse. Today, these voice user interfaces (VUI) are used for partially or fully automated telephone services provided by companies to customers, employees or partners. Business domains that rely heavily on VUIs include banking, supply chain, public transportation, and telecommunications. Other uses of speech interaction technology include interfaces to car navigation systems and the use of spoken language as an alternative to the graphical or touch-screen interfaces in smartphones. 

Speech interaction comprises four technologies: 

\begin{enumerate}
\item Automatic \uline{speech recognition} (ASR) determines which words are actually spoken in a given sequence of sounds uttered by a user.
\item Natural language understanding analyses the syntactic structure of a user’s utterance and interprets it according to the system in question.
\item Dialogue management determines which action to take given the user input and system functionality.
\item \uline{Speech synthesis} (text-to-speech or TTS) transforms the system’s reply into sounds for the user. 
\end{enumerate}

One of the major challenges of ASR systems is to accurately recognise the words a user utters. This means restricting the range of possible user utterances to a limited set of keywords, or manually creating language models that cover a large range of natural language utterances. Using machine learning techniques, language models can also be generated automatically from \uline{speech corpora}, i.e. large collections of speech audio files and text transcriptions. Restricting utterances usually forces people to use the voice user interface in a rigid way and can damage user acceptance; but the creation, tuning and maintenance of rich language models will significantly increase costs. VUIs that employ language models and initially allow a user to express their intent more flexibly — prompted by a \textit{How may I help you?} greeting — tend to be automated and are better accepted by users. 

Companies tend to use pre-recorded utterances by professional speakers for generating the output of the voice user interface. For static utterances where the wording does not depend on particular contexts of use or personal user data, this can deliver a rich user experience. But more dynamic content in an utterance may suffer from unnatural intonation because bits of audio files have simply been strung together. Today’s TTS systems are getting better (though they can still be optimised) at producing natural-sounding dynamic utterances.  

Interfaces in the market for speech interaction technologies have been considerably standardised during the last decade in terms of their various technology components. There has also been strong market consolidation in speech recognition and speech synthesis. The national markets in the G20 countries (economically resilient countries with high populations) have been dominated by just five global players, with Nuance (USA) and Loquendo (Italy) being the most prominent players in Europe. In 2011, Nuance announced the acquisition of Loquendo, which represents a further step in market consolidation.

On the Bulgarian TTS market, there are a few Bulgarian text-to-speech systems. One of these is SpeechLab 2.0 provided free-of-charge to computer users with visual disabilities. Only a few companies such as "Ciela" – a Bulgarian publisher of legal literature — have developed their own system for Bulgarian speech recognition. Finally, within the area of Speech Interaction, the is as yet no real market for syntactic and semantic analysis-based core technologies. 

The demand for voice user interfaces in Bulgaria has grown fast in the last five years, driven by increasing demand for customer self-service, cost optimisation for automated telephone services, and the increasing acceptance of spoken language as a media for human-machine interaction. 

Looking forward, there will be significant changes due to the spread of smartphones as a new platform for managing customer relationships in addition to fixed telephones, the Internet and e-mail. This will also affect how speech interaction technology is used. In the long run, there will be fewer telephone-based VUIs and spoken language will play a far more central role as a user-friendly input for smartphones. This will be largely driven by stepped improvements in the accuracy of speaker-independent speech recognition via speech dictation services already offered as centralised services to smartphone users. 

\subsubsection{Machine Translation}

The idea of using digital computers to translate natural languages goes back to 1946 and was followed by substantial funding for research during the 1950s and again in the 1980s. Yet \uline{machine translation} (MT) still cannot meet its initial promise of across-the-board automated translation.  

The most basic approach to machine translation is to automatically replace the words in a text in one natural language by words in another language. This can be useful in subject domains that have a very restricted, formulaic language such as weather reports. But to produce a good translation of less standardised texts, larger text units (phrases, sentences, or even whole passages) need to be matched to their closest counterparts in the target language. The major difficulty is that human language is ambiguous. Ambiguity creates challenges on multiple levels, such as word sense disambiguation on the lexical level ({\it a jaguar} is a brand of car or an animal) or the assignment of case on the syntactic level, for example:

{\it
\hspace{0.5cm}{Politsayat nablyudava prestapnika s tele-skopa.}
}

\hspace{0.5cm}[The policeman observed the man with the telescope.]

{\it
\hspace{0.5cm}{Politsayat nablyudava prestapnika s pistoleta.}
}

\hspace{0.5cm}[The policeman observed the man with the revolver.]


One way to build an MT system is to use linguistic rules. For translations between closely related languages, a direct substitution translation may be feasible in cases like the above example. But, rule-based (or linguistic knowledge-driven) systems often analyse the input text and create an intermediary symbolic representation from which the text can be generated into the target language. The success of these methods is highly dependent on the availability of extensive lexicons with morphological, syntactic, and semantic information, and large sets of grammar rules carefully designed by skilled linguists. This is a very long and therefore costly process.

In the late 1980s when computational power increased and became cheaper, there was more interest in statistical models for machine translation. Statistical models are derived from analysing bilingual text corpora, such as the Europarl parallel corpus, which contains the proceedings of the European Parliament in 11 European languages. Given enough data, statistical MT works well enough to derive an approximate meaning of a foreign language text by processing parallel versions and finding plausible patterns of words. But unlike knowledge-driven systems, statistical (or data-driven) MT often generates ungrammatical output. Data-driven MT is advantageous because less human effort is required, and it can also cover special particularities of the language (e.g., idiomatic expressions) that can get ignored in knowledge-driven systems. 

The strengths and weaknesses of knowledge-driven and data-driven machine translation tend to be complementary, so that nowadays researchers focus on hybrid approaches that combine both methodologies. One approach uses both knowledge-driven and data-driven systems together with a selection module that decides on the best output for each sentence. However, results for sentences longer than say 12 words will often be far from perfect. A better solution is to combine the best parts of each sentence from multiple outputs; this can be fairly complex, as corresponding parts of multiple alternatives are not always obvious and need to be aligned. 

For Bulgarian, MT is particularly challenging. The lack of noun case inflection; free word order and subject pro-drop pose problems for analysis. Extensive inflection in verb morphology is a challenge for generating words with proper markings. 

One of the good examples is WebTrance by SkyCode – a machine translation system which automatically translates texts, help files, menus, windows and internet pages from English, German, French, Spanish, Italian and Turkish into and from Bulgarian.  The use of machine translation can significantly increase productivity provided the system is intelligently adapted to user-specific terminology and integrated into a workflow. 

There is still a huge potential for improving the quality of MT systems. The challenges involve adapting language resources to a given subject domain or user area, and integrating the technology into workflows that already have term bases and translation memories. Another problem is that most of the current systems are English-centred and only support a few languages from and into Bulgarian. This leads to friction in the translation workflow and forces MT users to learn different lexicon coding tools for different systems.

Evaluation campaigns help compare the quality of MT systems, the different approaches and the status of the systems for different language pairs. The table below, which was prepared during the EC Euromatrix+ project, shows the pair-wise performances obtained for 22 of the 23 official EU languages (Irish was not compared). The results are ranked according to a BLEU score, which indicates higher scores for better translations \cite{bleu1}.  A human translator would achieve a score of around 80 points.

The best results (in green and blue) were achieved by languages that benefit from a considerable research effort in coordinated programs and from the existence of many parallel corpora (e.g., English, French, Dutch, Spanish and German). The languages with poorer results are shown in red. These languages either lack such development efforts or are structurally very different from other languages (e.g., Hungarian, Maltese and Finnish).

\subsection{Other Application Areas}

Building language technology applications involves a range of subtasks that do not always surface at the level of interaction with the user, but they provide significant service functionalities "under the hood" of the system in question. They all form important research issues that have now evolved into individual sub-disciplines of computational linguistics. 

Question answering, for example, is an active area of research for which annotated corpora have been built and scientific competitions have been initiated. The concept of question answering goes beyond keyword-based searches (in which the search engine responds by delivering a collection of potentially relevant documents) and enables users to ask a concrete question to which the system provides a single answer. For example:

\hspace{0.5cm}Question: {\it How old was Neil Armstrong when he stepped on the moon?}

\hspace{0.5cm}Answer: {\it 38.}

While question answering is obviously related to the core area of web search, it is nowadays an umbrella term for such research issues as what different types of questions there are, and how they should be handled; how a set of documents that potentially contain the answer can be analysed and compared (do they provide conflicting answers?); and how specific information (the answer) can be reliably extracted from a document without ignoring the context. 

This is in turn related to information extraction (IE), an area that was extremely popular and influential when computational linguistics took a statistical turn in the early 1990s. IE aims to identify specific pieces of information in specific classes of documents, such as detecting the key players in company takeovers as reported in newspaper stories. Another common scenario that has been studied is reports on terrorist incidents. The problem here is to map the text to a template that specifies the perpetrator, target, time, location and results of the incident. Domain-specific template-filling is the central characteristic of IE, which makes it another example of a "behind the scenes" technology that forms a well-demarcated research area that in practice needs to be embedded into a suitable application environment. 

Text summarisation and text generation are two borderline areas that can act either as standalone applications or play a supporting role "under the hood". Summarisation attempts to give the essentials of a long text in a short form, and is one of the features available in Microsoft Word. It mostly uses a statistical approach to identify the "important" words in a text (i.e., words that occur very frequently in the text in question but less frequently in general language use) and determine which sentences contain the most of these "important" words. These sentences are then extracted and put together to create the summary. In this very common commercial scenario, summarisation is simply a form of sentence extraction, and the text is reduced to a subset of its sentences. An alternative approach, for which some research has been carried out, is to generate brand new sentences that do not exist in the source text. This requires a deeper understanding of the text, which means that
  so far this approach is far less robust. On the whole, a text generator is rarely used as a stand-alone application but is embedded into a larger software environment, such as a clinical information system that collects, stores and processes patient data. Creating reports is just one of many applications for text summarisation. 

For the Bulgarian language, research in these text technologies is much less developed than for the English language. Question answering, information extraction, and summarisation have been the focus of numerous open competitions in the USA since the 1990s, primarily organised by the government-sponsored organisations DARPA and NIST. These competitions have significantly improved the start-of-the-art, but their focus has mostly been on the English language. As a result, there are hardly any annotated corpora or other special resources needed to perform these tasks in Bulgarian. When summarisation systems use purely statistical methods, they are largely language-independent and a number of research prototypes are available. For text generation, reusable components have traditionally been limited to surface realisation modules (generation grammars) and most of the available software is for the English language. 

\subsection{Educational Programmes}

Language technology is a very interdisciplinary field that involves the combined expertise of linguists, computer scientists, mathematicians, philosophers, psycholinguists, and neuroscientists among others. As a result, it has not acquired a clear, independent existence in the Bulgarian faculty system. In Bulgarian universities courses in computational linguistics are only partially available. They are usually designed either for humanities students or mathematicians but not both. Two years ago the University of Plovdiv began to offer a Bachelors programme in Linguistics with Information Technologies. 30 students enrolled in 2009-2010 and twice that number in 2010-2011. Students are offered a wide range of courses connected with the fundamentals of linguistics, mathematics and programming as well as language technology applications. The Bachelors degree in Informatics at the University of Plovdiv traditionally offers a lecture course in Computational Linguistics. 

Since 2004 the Faculty of Mathematics and Informatics of the University of Sofia has been offering a Masters programme in Artificial Intelligence. The University of Sofia also offers a Masters programme in Computational Linguistics. The programme includes subjects from the sphere of mathematics, logic, programming and theoretical linguistics. Graduates possess a good grounding to begin academic research in the area of computational linguistics as well as broader computational and linguistic literacy allowing them ability to develop unconventional practical applications. The success of the programme is evident from the professional development of the students from the previous intakes. They are employed in leading IT companies, popular national and specialised media and particularly in academic research.
The education of language technology in sufficient numbers is a prerequisite for the diverse research and thus the development of successful commercial activity.

\subsection{National Projects and Efforts}

The first international and national funding supporting language technologies for Bulgarian began at the very beginning of 1990s. Over a short period of time financing for a number of research projects from European institutions was got: LaTeSLav (Language Processing Technologies for Slavic Languages, 1991-1994) – aimed at developing a prototype of a grammar checker, BILEDITA (Bilingual Electronic Dictionaries and Intelligent Text Alignment, 1996-1998) – for the development of bi-lingual electronic dictionaries, GLOSSER (Support of Second Language Acquisition and Learning from Aligned Corpora, 1996-1998) – aimed at supporting foreign language training and others. The Multext-East (Multilingual Text Tools and Corpora for Central and Eastern European Languages, 1995-1997) extension of the previous Multext and EAGLES (European Commission's Expert Advisory Group on Language Engineering Standards) EU projects provided the Bulgarian language resources in a standardised format
  with standard mark-up and annotation, and these resources were later expanded and upgraded in the ELAN (European Language Activity Network, 1998-1999), TELRI I in II (Trans European Language Resources Infrastructure, 1995-1998 / 1999-2001) and Concede (Consortium for Central European Dictionary Encoding, 1998-2000) projects.

The Ministry of Education, Youth and Research through the National Scientific Fund (NSF), has supported research through national research programs. These programs have impelled numerous research projects and collaboration with international research centres and companies. The basis of technology development and commercial applications for automated processing of the Bulgarian language has been partly created as a result of these projects.

A number of years ago five Bulgarian academic institutions founded a consortium to create and develop an integrated national academic infrastructure for language resources. Bulgarian institutions are also involved in the CLARIN project (Common Language Resources and Technology Infrastructure). Other ongoing projects include those comprised by EUROPEANA aimed at developing the basic technologies and standards necessary to make knowledge on the Internet more widely available in the future. In addition to many other smaller-scale funded projects, the above-mentioned projects have led to the development of competences in the field of Language Technology as well as a basic technological infrastructure of language tools and resources for Bulgarian. However, public funding for LT projects in Bulgaria is dramatically lower than that for comparable projects in Europe, as well as in comparison to investments into areas such as language translation and multilingual information access by
  the USA \cite{sprachtech}.  

\subsection{Availability of Tools and Resources}

The following table summarises the current state of language technology support for the Bulgarian language. The rating for existing tools and resources was generated by leading experts in the field who provided estimates based on a scale from 0 (very low) to 6 (very high) according to seven criteria.

\begin{figure*}[htb]
\centering
%\begin{tabular}{>{\columncolor{orange1}}p{.33\linewidth}ccccccc} % ORIGINAL
\begin{tabular}{>{\columncolor{orange1}}p{.33\linewidth}@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{6mm}}c}
\rowcolor{orange1}
 \cellcolor{white}&\begin{sideways}\makecell[l]{Quantity}\end{sideways}
&\begin{sideways}\makecell[l]{\makecell[l]{Availability} }\end{sideways} &\begin{sideways}\makecell[l]{Quality}\end{sideways}
&\begin{sideways}\makecell[l]{Coverage}\end{sideways} &\begin{sideways}\makecell[l]{Maturity}\end{sideways} &\begin{sideways}\makecell[l]{Sustainability}\end{sideways} &\begin{sideways}\makecell[l]{Adaptability}\end{sideways} \\ \addlinespace
\multicolumn{8}{>{\columncolor{orange2}}l}{Language Technology: Tools, Technologies and Applications} \\ \addlinespace
Speech Recognition	&	1.6 &	0.8 &	2.4 &	2.4 &	1.6 &	1.6 &	0.8 \\ \addlinespace
Speech Synthesis &	1.6 &	0.8 &	2.4 &	2.4 &	1.6 &	1.6 &	0.8\\ \addlinespace
Grammatical analysis &	2.4 &	2 &	3.6 &	3.6 &	2.8 &	2.4 &	2.8\\ \addlinespace
Semantic analysis &	0.8 &	0.8 &	1.3 &	1.1 &	0.8 &	1.1 &	1.3\\ \addlinespace
Text generation &	0.8 &	0.8 &	1.6 &	1.6 &	1.6 &	0.8 &	0.8\\ \addlinespace
Machine translation &	2.4 &	1.6 &	1.6 &	1.6 &	1.6 &	1.6 &	2.4\\ \addlinespace
\multicolumn{8}{>{\columncolor{orange2}}l}{Language Resources: Resources, Data and Knowledge Bases} \\ \addlinespace
Text corpora &	2.8 &	2.4 &	3.2 &	2.4 &	3.2 &	2.4 &	2.8\\ \addlinespace
Speech corpora &	0.8 &	0.8 &	2.4 &	1.6 &	2.4 &	2.4 &	2.5\\ \addlinespace
Parallel corpora &	2.4 &	0.8 &	3.2 &	1.6 &	1.6 &	1.6 &	2.4\\ \addlinespace
Lexical resources &	2.4 &	2.8 &	3.6 &	2.8 &	3.2 &	3.2 &	3.2\\ \addlinespace
Grammars &	1.6 &	1.6 &	2.4 &	2.4 &	2.4 &	2.4 &	1.6\\
\end{tabular}
\caption{State of language technology support for Bulgarian}
\label{fig:lrlttable_en}
\end{figure*}

The key results for the Bulgarian language can be summed up as follows:

\begin{itemize}
\item The more linguistic and semantic knowledge a tool draws on, the more gaps there are in the technology (information retrieval versus text semantics). There is a need for far more effort to support deep linguistic processing.

\item Research has been successful in designing particularly high quality software, but many of the resources are not standardised and cannot be sustained effectively. A concerted program is required to standardise data and interchange formats.

\item While some reference corpora of high quality and quantity exist, i.e. Bulgarian national corpus, large syntactically and semantically corpora annotated by experts are not available.  Again, the situation gets worse as the need for more deep linguistic and semantic information grows.

\item There is a lack of parallel corpora for machine translation. Translation from Bulgarian to English works best because this language pair has the most data available. 

\item There is a huge gap in multimedia data.
\end{itemize}

\subsection{Cross-language comparison}

The languages were clustered using the following five-point scale: 

    \begin{itemize}
      \item Cluster 1 (excellent LT support)
      \item Cluster 2 (good support)
      \item Cluster 3 (moderate support)
      \item Cluster 4 (fragmentary support) 
      \item Cluster 5 (weak or no support)
    \end{itemize}

LT support was measured according to the following criteria:

\begin{itemize}
\item Speech Processing: Quality of existing speech recognition technologies, quality of existing speech synthesis technologies, coverage of domains, number and size of existing speech corpora, amount and variety of available speech-based applications
\item Machine Translation: Quality of existing MT technologies, number of language pairs covered, coverage of linguistic phenomena and domains, quality and size of existing parallel corpora, amount and variety of available MT applications
\item Text Analysis: Quality and coverage of existing text analysis technologies (morphology, syntax, semantics), coverage of linguistic phenomena and domains, amount and variety of available applications, quality and size of existing (annotated) text corpora, quality and coverage of existing lexical resources (e.g., WordNet) and grammars
\item Resources: Quality and size of existing text corpora, speech corpora and parallel corpora, quality and coverage of existing lexical resources and grammars
\end{itemize} 

\subsection{Conclusions}

In this series of white papers, we have made an important
initial effort to assess language technology support for 30 European
languages, and provide a high-level comparison across these
languages. By identifying the gaps, needs and deficits, the European
language technology community and related stakeholders are now in a
position to design a large scale research and development programme
aimed at building a truly multilingual, technology-enabled Europe.

We have seen that there are huge differences between Europe’s
languages. While there are good quality software and resources
available for some languages and application areas, others (usually
‘smaller’ languages) have substantial gaps.  Many languages lack basic
technologies for text analysis and the essential resources for
developing these technologies. Others have basic tools and resources
but are as yet unable to invest in semantic processing. We therefore
still need to make a large-scale effort to attain the ambitious goal
of providing high-quality machine translation between all European
languages.

Over the past decade a number of important electronic language resources for Bulgarian (dictionaries, corpora, lexical data bases) as well as programmes for their processing (word sense disambiguation tool, spell checking, etc..) have been developed. However, the scope of the resources and the range of tools are still very limited when compared to the resources and tools for the English language, and they are simply not sufficient in quality and quantity to develop the kind of technologies required to support a truly multilingual knowledge society.

Nor can we simply transfer technologies already developed and optimised for the English language to handle Bulgarian. English-based systems for parsing (syntactic and grammatical analysis of sentence structure) typically perform far less well on German texts, due to the specific characteristics of the Bulgarian language such as free word order or sibject ommision.
The Bulgarian language technology industry dedicated to transforming research into products is currently fragmented and disorganised. A number of specialised small and middle SMEs that are not robust enough to address the internal and the global market with a sustained strategy are working in the field.

  Our findings show that the only alternative is to make a substantial effort to create LT resources for Bulgarian, and use them to drive forward research, innovation and development. The need for large amounts of data and the extreme complexity of language technology systems makes it vital to develop a new infrastructure and a more coherent research organization to spur greater sharing and cooperation.

There is also a lack of continuity in research and development funding. Short-term coordinated programmes tend to alternate with periods of sparse or zero funding. In addition, there is an overall lack of coordination with programmes in other EU countries and at the European Commission level.
In general, it can be stated that in the last two decades language technology for Bulgarian was never supported by a consistently devised national funding scheme. The process of development of HLT applications, tools and resources for Bulgarian has been, therefore, a mixture of international projects extending their scope from Western European languages to Middle and Eastern Europe, also with a view of the EU enlargement process, national research funding, and the enthusiasm of researchers involved in LT.

We can therefore conclude that there is a desperate need for a large, coordinated initiative focused on overcoming the differences in language technology readiness for European languages as a whole. 

Bulgaria’s participation in
META-NET will make it possible to develop, standardise and make
available several important LT resources and thus contribute to the
growth of Bulgarian language technology.

META-NET’s long-term goal is to introduce high-quality language technology for all languages in order to achieve political and economic unity through cultural diversity. The technology will help tear down existing barriers and build bridges between Europe’s languages. This requires all stakeholders — in politics, research, business, and society — to unite their efforts for the future.
\end{multicols}

\clearpage

\ssection[About META-NET]{About META-NET}

\begin{multicols}{2}
                                                                                                                                                                                                                                                                                                                                                META-NET is a Network of Excellence funded by the European Commission. The network currently consists of 47 members from 31 European countries. META-NET fosters the Multilingual Europe Technology Alliance (META), a growing community of language technology professionals and organisations in Europe. 

META-NET cooperates with other initiatives like the Common Language Resources and Technology Infrastructure (CLARIN), which is helping establish digital humanities research in Europe. META-NET fosters the technological foundations for a truly multilingual European information society that:

\begin{itemize}
\item makes communication and cooperation possible across languages;
\item provides equal access to information and knowledge in any language;
\item offers advanced and affordable networked information technology to European citizens.
\end{itemize}

META-NET stimulates and promotes multilingual technologies for all European languages. The technologies enable automatic translation, content production, information processing and knowledge management for a wide variety of applications and subject domains. The network wants to improve current approaches, so better communication and cooperation across languages can take place. Europeans have an equal right to information and knowledge regardless of language. 

META-NET launched on 1 February 2010 with the goal of advancing research in language technology. The network supports a Europe that unites as a single digital market and information space. META-NET has conducted several activities that further its goals. META-VISION, META-SHARE and META-RESEARCH are the network’s three lines of action.

META-VISION fosters a dynamic and influential stakeholder community that unites around a shared vision and a common strategic research agenda (SRA). The main focus of this activity is to build a coherent and cohesive LT community in Europe by bringing together representatives from highly fragmented and diverse groups of stakeholders. In the first year of META-NET, presentations at the FLaReNet Forum (Spain), Language Technology Days (Luxembourg), JIAMCATT 2010 (Luxembourg), LREC 2010 (Malta), EAMT 2010 (France) and ICT 2010 (Belgium) centred on public outreach. According to initial estimates, META-NET has already contacted more than 2,500 LT professionals to develop its goals and visions with them. At the META-FORUM 2010 event in Brussels, META-NET communicated the initial results of its vision building process to more than 250 participants. In a series of interactive sessions, the participants provided feedback on the visions presented by the network. 

META-SHARE creates an open, distributed facility for exchanging and sharing resources. The peer-to-peer network of repositories will contain language data, tools and web services that are documented with high-quality metadata and organised in standardised categories. The resources can be readily accessed and uniformly searched. The available resources include free, open source materials as well as restricted, commercially available, fee-based items. META-SHARE targets existing language data, tools and systems as well as new and emerging products that are required for building and evaluating new technologies, products and services. The reuse, combination, repurposing and re-engineering of language data and tools plays a crucial role. META-SHARE will eventually become a critical part of the LT marketplace for developers, localisation experts, researchers, translators and language professionals from small, mid-sized and large enterprises. META-SHARE addresses the full developmen
 t cycle of LT—from research to innovative products and services. A key aspect of this activity is establishing META-SHARE as an important and valuable part of a European and global infrastructure for the LT community. 

META-RESEARCH builds bridges to related technology fields. This activity seeks to leverage advances in other fields and to capitalise on innovative research that can benefit language technology. In particular, this activity wants to bring more semantics into machine translation (MT), optimise the division of labour in hybrid MT, exploit context when computing automatic translations and prepare an empirical base for MT. META-RESEARCH is working with other fields and disciplines, such as machine learning and the Semantic Web community. META-RESEARCH focuses on collecting data, preparing data sets and organising language resources for evaluation purposes; compiling inventories of tools and methods; and organising workshops and training events for members of the community. This activity has already clearly identified aspects of MT where semantics can impact current best practices. In addition, the activity has created recommendations on how to approach the problem of integrating 
 semantic information in MT. META-RESEARCH is also finalising a new language resource for MT, the Annotated Hybrid Sample MT Corpus, which provides data for English-German, English-Spanish and English-Czech language pairs. META-RESEARCH has also developed software that collects multilingual corpora that are hidden on the Web.
\end{multicols}
